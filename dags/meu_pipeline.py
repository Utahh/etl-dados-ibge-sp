from airflow import DAG
from airflow.operators.bash import BashOperator
from airflow.operators.python import PythonOperator
from datetime import datetime, timedelta
import polars as pl
import os
import json
from sqlalchemy import create_engine

# --- ConfiguraÃ§Ãµes ---
# Caminhos internos do Docker (NÃƒO ALTERE PARA D:\...)
NODE_PROJECT_PATH = "/opt/node_project"
JSON_SOURCE = f"{NODE_PROJECT_PATH}/output/consolidado_municipios.DS0.json"
CSV_DESTINO = "/opt/airflow/data/consolidado_municipios_final.csv"

# ConexÃ£o com o Postgres (definida no docker-compose)
DB_CONNECTION_URI = "postgresql+psycopg2://admin_dados:senha_secreta@postgres:5432/dw_orquestrador"

def processar_json_para_csv():
    """LÃª o JSON gerado pelo Node, trata com Polars e salva CSV"""
    print("ðŸš€ Iniciando processamento com Polars...")
    
    try:
        with open(JSON_SOURCE, 'r', encoding='utf-8') as f:
            raw_data = json.load(f)
            
        # LÃ³gica de extraÃ§Ã£o segura (baseada no seu cÃ³digo original)
        first_key = list(raw_data.keys())[0] if isinstance(raw_data, dict) else None
        list_data = raw_data[first_key] if first_key else raw_data
        
        df = pl.DataFrame(list_data)

        # Adiciona colunas de referÃªncia (Exemplo estÃ¡tico, idealmente dinÃ¢mico)
        df = df.with_columns([
            pl.lit(datetime.now().year).alias("Ano_Ref"),
            pl.lit("Outubro").alias("Mes_Ref") # Ajustar lÃ³gica de mÃªs se necessÃ¡rio
        ])

        # Renomeia colunas
        if "GeogrÃ¡fico.MunicÃ­pio" in df.columns:
            df = df.rename({"GeogrÃ¡fico.MunicÃ­pio": "MunicÃ­pio"})

        # Salva CSV
        df.write_csv(CSV_DESTINO, separator=";")
        print(f"âœ… CSV salvo em: {CSV_DESTINO}")
        
    except Exception as e:
        print(f"âŒ Erro no processamento: {e}")
        raise e

def carregar_para_banco():
    """LÃª o CSV e insere na tabela SQL"""
    print("ðŸ’¾ Iniciando carga no Banco de Dados...")
    
    df = pl.read_csv(CSV_DESTINO, separator=";")
    
    # Cria conexÃ£o
    engine = create_engine(DB_CONNECTION_URI)
    
    # Escreve no banco (modo 'replace' recria a tabela, 'append' adiciona)
    # Vamos usar Pandas para o write_sql pela facilidade de compatibilidade inicial
    df_pandas = df.to_pandas()
    df_pandas.to_sql('tb_municipios', engine, if_exists='replace', index=False)
    
    print("âœ… Dados carregados no PostgreSQL com sucesso!")

# --- DefiniÃ§Ã£o da DAG ---
default_args = {
    'owner': 'airflow',
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}

with DAG(
    dag_id='orquestrador_municipios_v1',
    default_args=default_args,
    description='Pipeline completo: Node -> Polars -> Postgres',
    start_date=datetime(2023, 1, 1),
    schedule_interval='0 8 * * *', # Roda todo dia Ã s 08:00 da manhÃ£
    catchup=False
) as dag:

    # Tarefa 1: Executa o comando npm run start:json
    t1_extracao = BashOperator(
        task_id='extracao_node',
        bash_command=f"cd {NODE_PROJECT_PATH} && npm run start:json"
    )

    # Tarefa 2: Python trata os dados
    t2_transformacao = PythonOperator(
        task_id='transformacao_polars',
        python_callable=processar_json_para_csv
    )

    # Tarefa 3: Carrega no Banco
    t3_carga_banco = PythonOperator(
        task_id='carga_postgres',
        python_callable=carregar_para_banco
    )

    # Ordem de execuÃ§Ã£o
    t1_extracao >> t2_transformacao >> t3_carga_banco